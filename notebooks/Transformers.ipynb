{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbbcee6",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c67b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c325d67",
   "metadata": {},
   "source": [
    " ![Transformer](./assets/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc6715",
   "metadata": {},
   "source": [
    "The transformer architecture consist of two main Blocs:\n",
    "\n",
    "- `Encoder`: consist of multiple identical layers that are responsible for reading and processing the input sequence and generate context rich nemurical representations. It doing this using **self-attention** and **feed-forward** networks.\n",
    "\n",
    "- `Decoder`: Essentialy do the inverse of the `encoder` bloc by generating an output sequence based on the encoded input sequence which is coming from the `encoder`.\n",
    "\n",
    "- `Positional encoding`: included in both blocks, it allows tokens be processed in parallel by encoding each token postion in the sequence. This allows the model to recognize the relationships between tokens and their order (Essential for making sense in sentences and capture the context).\n",
    "\n",
    "- `Attention Mechanisms`: used to highlight the most important tokens and their relationships which improves the quality of the generated text.\n",
    "\n",
    "- `Self-attention`: is type of `Attention Mechanisms` that assigns a weight to each token in the sequence similtanuously capturing long-range dependencies.\n",
    "\n",
    "- `Multi-Head attention`: extends `Self-attention` by using multiple heads to focus on different aspect of the input sequence in parallel. This allows each head to capture distinct relational patterns within the data leading to richer representations.\n",
    "\n",
    "- `Position-wise feed-forward networks`: this simple FFN that apply complex transformations on each tokens embeddings independently. Because each token get it's own transformation, the networks are position independent (`Position wise`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fed12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ia/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Transformer(\n",
    "    d_model=128,\n",
    "    nhead=4,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc32689",
   "metadata": {},
   "source": [
    "# Embedding and positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293badc9",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aadb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size:int, d_model:int) ->None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cbc567",
   "metadata": {},
   "source": [
    "`math.sqrt(self.d_model)` this scaling is a trick from the original Transformer paper. Why?:\n",
    "\n",
    "- The embeddings are initially small random values.\n",
    "\n",
    "- Scaling by $ \\sqrt(d_model) $ increases their magnitude so that their variance matches that of the positional encodings (added later), helping with stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5b37a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 128]), torch.Size([2, 6, 128]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = InputEmbeddings(vocab_size=10, d_model=128)\n",
    "sentence = [0,1,2,3,4,5,6,7,1,1,1,1] # each Token ID in the sentence ≤ vocab_size - 1 \n",
    "batch_sentences= [\n",
    "    [1,2,3,4,4,5], \n",
    "    [5,6,7,1,3,4]\n",
    "] # (batch_size, sequence_length, d_model) , sequence length must be same in the one batch \n",
    "\n",
    "x = torch.tensor(sentence)\n",
    "z = torch.tensor(batch_sentences)\n",
    "embedded_output_sentence = embedding_layer(x)\n",
    "embedded_output_batch = embedding_layer(z)\n",
    "embedded_output_sentence.shape , embedded_output_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8c526",
   "metadata": {},
   "source": [
    "- Batch sentences need to have the same length cz when we create a PyTorch tensor for a batch, all sequences must have the same length to form a proper rectangular tensor (matrix).\n",
    "\n",
    "- If sequences have different lengths we `pad` the shorter sequences with a special token (e.g., token ID 0) to the max length in the batch.\n",
    "\n",
    "- When do you need to pad?\n",
    "\n",
    "    - When you want to process multiple sequences together in a batch (e.g., batch size > 1) and these sequences have different lengths.\n",
    "\n",
    "    - PyTorch tensors require all sequences in the batch to have the same length to form a single tensor.\n",
    "\n",
    "    - So, you pad shorter sequences with a special token (e.g., token ID 0) to match the longest sequence length in the batch.\n",
    "\n",
    "-  When can you skip padding?\n",
    "\n",
    "    - If you’re processing sequences one by one (batch size = 1), no padding is necessary.\n",
    "\n",
    "    - If all sequences in your batch already have the same length, no padding is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474afb3e",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "- Encode each token's position in the sequence into `Positional embedding` and adds them into the `Token embeddings` to capture the positional information.\n",
    "\n",
    "- The token and postional embedding usally have the same dimensionality for easy readation.\n",
    "\n",
    "- The positional embedding is generated using an equation.\n",
    "\n",
    "- $ \\text{Input}_i = \\text{TokenEmbedding}_i + \\text{PositionalEncoding}_i $\n",
    "\n",
    "- $ PE_{pos, 2i} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $\n",
    "\n",
    "- $ PE_{pos, 2i+1} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right) $\n",
    "\n",
    "**Why use sine and cosine functions for positional encoding?**\n",
    "\n",
    "- Periodic functions encode positions smoothly.\n",
    "\n",
    "- By assigning each position a vector of sine and cosine values at different frequencies, we get a unique “signature” for each position.\n",
    "\n",
    "- These signatures vary smoothly as position changes, so positions close to each other have similar encodings, and positions far apart have more different encodings.\n",
    "\n",
    "**Why a vector, not a scalar?**\n",
    "\n",
    "- A single number (scalar) wouldn’t capture complex position info.\n",
    "\n",
    "- The vector’s different components capture patterns at multiple frequencies, encoding both fine and coarse positional details.\n",
    "\n",
    "- This multi-dimensional encoding lets the Transformer distinguish many positions and compute relationships between them.\n",
    "\n",
    "\n",
    "**Positional encoding vector**\n",
    "\n",
    "- The dimension of each positional encoding vector is the same as the model’s embedding size, usually called `d_model`.\n",
    "\n",
    "- So for $$ PE(pos) \\in \\mathbb{R}^{d_{model}} $$ If $ d_{model}=128 $, then $ PE(pos) = [PE_0, PE_1, …, PE_{127}]  $ where each $PE_i$ is computed by the sine or cosine formula.\n",
    "\n",
    "- Why the same dimension?\n",
    "\n",
    "    - Because you add the positional encoding vector to the token embedding vector element-wise.\n",
    "\n",
    "    - For addition to work, both vectors must have the same shape, i.e., (d_model,).\n",
    "\n",
    "**Positional Encoding matrix shape**\n",
    "\n",
    "- Suppose your maximum sequence length is `max_len` (e.g., 512 tokens max in a sentence).\n",
    "\n",
    "- Your model dimension (embedding size) is `d_model` (e.g., 512).\n",
    "\n",
    "- The positional encoding matrix has shape: $$ ( max_{len},  d_{model}) $$\n",
    "\n",
    "- Each row corresponds to one position (from 0 to max_len - 1).\n",
    "\n",
    "- Each column corresponds to one dimension of the positional encoding vector.\n",
    "\n",
    "\n",
    "**Does the positional encoding vector change during training?**\n",
    "\n",
    "- Original Transformer’s sinusoidal positional encoding fixed.\n",
    "\n",
    "- Some Transformer variants replace sinusoidal encoding with learnable positional embeddings (just like token embeddings, but for positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66268da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_sequence_len:int):\n",
    "        super().__init__()\n",
    "        positional_encoding = torch.zeros(max_sequence_len, d_model)\n",
    "        position = torch.arange(0, max_sequence_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer(\"positional_encoding\", positional_encoding.unsqueeze(0))\n",
    "\n",
    "        # register_buffer stores the positional encoding tensor in the module without making it a learnable parameter.\n",
    "        # .unsqueeze(0) adds a batch dimension → shape becomes (1, max_sequence_len, d_model) so it can be broadcast over batches during addition.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.positional_encoding[:, :x.size(1)]  \n",
    "      \n",
    "    # x.size(1) is the current sequence length (number of tokens) in the batch — could be smaller or equal to max_sequence_len.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e54d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 4\n",
    "embedding_layer = InputEmbeddings(vocab_size=10, d_model=128)\n",
    "input_sentence = [\n",
    "    [1,2,3,4], # his seq length must be <= max_seq_len and since we are in batch all sequences must be the same length\n",
    "    [5,6,7,1],\n",
    "] \n",
    "input_sentence = torch.tensor(input_sentence)\n",
    "embedded_output = embedding_layer(input_sentence)\n",
    "pe_layer = PositionalEncoding(d_model=128, max_sequence_len=4)\n",
    "pe_output = pe_layer(embedded_output)\n",
    "pe_output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21022cf",
   "metadata": {},
   "source": [
    "# Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c4a4c",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecc11f",
   "metadata": {},
   "source": [
    "- `Self-attention`: allow transformers to identify the relashionships between tokens and focus on the most relevant ones with the task.\n",
    "\n",
    "- Given a sequence of token embeddings, each embedding is projected into 3 matrices: `Q`, `K`, `V` of equal dimensions using separate linear transformations with learned weights.\n",
    "\n",
    "    - `Q: Query matrix`: indicate what each token is looking for in other tokens. “What am I looking for?”\n",
    "\n",
    "    - `K: Key matrix`: represent the content of each token that other tokens might find relevant. “What do I have that others might want?”\n",
    "\n",
    "    - `V: Value matrix`: Actual content to be aggregated or weighted based on the attention sccores. “The actual information I can give.”\n",
    "\n",
    "- Transform each token embedding into this rows helps the model learns more nuanced token relationships. \n",
    "\n",
    "- Attention scores are computed by the dot product of `Q` and `K` matrices. $$\\text{scores} = Q K^\\top$$\n",
    "\n",
    "- This measures how much each token’s query matches every token’s key.\n",
    "\n",
    "- Attention weights are computed by applying a softmax function to the attention scores.\n",
    "\n",
    "    - Apply softmax to the scores along each row → weights sum to 1.\n",
    "\n",
    "    - This gives “how much attention” each token should pay to every other token.\n",
    "\n",
    "\n",
    "- The attention weights reflect the relevance (attention) the model assigns to each token in the sequence.\n",
    "\n",
    "- Finally we multiply the attention weights with the `V` matrix (which are the token embeddings) to update the token embeddings with the self-attention information.\n",
    "\n",
    "- This attention mechanism use one attention head with one set of `Q`, `K`, `V` matrices. But in practice, embedding are split into multiple heads to focus on different aspects of the input sequence in parallel.\n",
    "\n",
    "- Multi-Head Attention concatenate the output of each head and linearly transform them to match the input dimensions(embedding dimension).\n",
    "\n",
    "- The resulting embedding captur token meaning, postional enconding and contextual relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e457b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int):\n",
    "        super().__init__()\n",
    "        assert d_model % nhead == 0\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // nhead # embedding dimension per head\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model,bias=False)\n",
    "        self.key_linear = nn.Linear(d_model, d_model,bias=False)\n",
    "        self.value_linear = nn.Linear(d_model, d_model,bias=False)\n",
    "\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "\n",
    "\n",
    "    def split_heads(self, x,batch_size):  \n",
    "        seq_length = x.size(1)\n",
    "        x = x.reshape(batch_size, seq_length, self.nhead, self.head_dim)  \n",
    "        x = x.permute(0, 2, 1, 3)  # (batch_size, nhead, seq_length, head_dim)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def compute_attention(self, query, key, value,mask=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        return torch.matmul(attention_weights, value)\n",
    "    \n",
    "    def combine_heads(self,x,batch_size):\n",
    "       \n",
    "        # x: (batch_size, nhead, seq_length, head_dim)\n",
    "\n",
    "        seq_length = x.size(2)\n",
    "       \n",
    "        # Move heads back to last dimension: (batch_size, seq_length, nhead, head_dim)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Merge nhead and head_dim back into d_model\n",
    "        x = x.reshape(batch_size, seq_length, self.d_model)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        query = self.split_heads(self.query_linear(query),batch_size)\n",
    "        key = self.split_heads(self.key_linear(key),batch_size)\n",
    "        value = self.split_heads(self.value_linear(value),batch_size)\n",
    "\n",
    "        attention_output = self.compute_attention(query,key,value,mask)\n",
    "\n",
    "        output = self.combine_heads(attention_output,batch_size)\n",
    "\n",
    "        return self.out_linear(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec68974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = MultiHeadAttention(d_model=128, nhead=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d428d",
   "metadata": {},
   "source": [
    "# Encoder-Decoder (Full transformer)\n",
    "\n",
    " ![Transformer2](./assets/original_transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dccdd6",
   "metadata": {},
   "source": [
    "# Encoder only transformers\n",
    "\n",
    "![Encoder](./assets/encoder.png)\n",
    "\n",
    "\n",
    "- Focus on understanding and representing the input data such as text classification.\n",
    "\n",
    "- Consists of two main components: \n",
    " \n",
    "   - **Transformer body**\n",
    "\n",
    "   - **Transformer head**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ead89",
   "metadata": {},
   "source": [
    "**Transformer body**:\n",
    "- Or encoder is stack of $N$ encoder layers designed to learn complex patterns from input data.\n",
    "\n",
    "- Each encoder layer incorporates:\n",
    "\n",
    "   - A multi-head self-attention mechanism to captures the relationships between tokens in the input sequence.\n",
    "\n",
    "   - Followed by feed-forward network (sublayer) to map this knowledge into abstract non-linear representations.\n",
    "\n",
    "   - Both of these components are usually combined with other techniques like layer normalizations and dropout to improve training.\n",
    "\n",
    "**Transformer head**: \n",
    "\n",
    "- Is the final layer designed to produce task-specific output.\n",
    "\n",
    "- Process encoded inputs to prediction outputs (classification, regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2b441",
   "metadata": {},
   "source": [
    "## Feed-Forward sublayer in encoder layer\n",
    "\n",
    "- Contains two fully connected linear layer with ReLU activation in between.\n",
    "\n",
    "- `d_ff` is the dimension of the hidden layer in the feed-forward sublayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b5e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c53823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, nhead: int, d_ff: int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "\n",
    "        atten_output = self.self_attn(x,x,x,mask)\n",
    "        atten_output = self.dropout1(atten_output)\n",
    "        x = self.norm1(x + atten_output)\n",
    "\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + feed_forward_output)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773f50c",
   "metadata": {},
   "source": [
    "- When we call attention mechanism in forward pass input embedding is passed as q,v,k.\n",
    "\n",
    "- Mask is used to prevent processing of padding tokens in the input sequence.\n",
    "\n",
    "- The padded tokens are irrelevant to the attention task so we exclude them from the attention machnism.\n",
    "\n",
    "- We do that by applying a padding mask to the attention scores so scores linked to padded tokens are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8f79f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size,d_model,num_layers,num_heads,d_ff,dropout,max_seq_length):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbeddings(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "                \n",
    "                ] )\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x,mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfd2e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_model: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc(x)\n",
    "        output = F.log_softmax(logits, dim=-1)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, d_model: int,output_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.fc(x)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
